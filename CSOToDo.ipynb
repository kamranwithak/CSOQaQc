{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c798c80-8091-4f85-8ef4-9b7215151308",
   "metadata": {},
   "source": [
    "## What's next for our exploration?\n",
    "\n",
    "There's still many things that need to be done in regards to our zero values, some of these were out of my league before but now that I've got more time we could maybe get into the nitty gritty of it.\n",
    "\n",
    "* It seems as though we most certainly want to include 0 values as they are without a doubt important for the data but something is for sure going wrong with the some of our SnowPilot data. \n",
    "\n",
    "* I can begin on a new test for much of the data, whether that be time reliant comparisons against SNODAS or to our recorded model. This could also be done in the form of climatologies but either of these are pretty heavy duty projects. This will require some discussion as from the recent developement from our simulations it can be almost \"guaranteed\" that our values are actually closer to satellite imagery than SNODAS. This still is very in the air as this is only in the Wyoming case.\n",
    "\n",
    "* I could transition towards focusing on some form of data visualizaiton with projections of some models. This could be something I can do after Dave explains his methodology for how he created his Wyoming model. I can also maybe just do some interesting Data visulization againest known models or known snow depth data.\n",
    "\n",
    "* An off shoot of this could be doing some discriptive statistics for the data. We could see what are some important factors for determining the depth of snow using multiple confidence and correlation tests. Some obvious things are going to be elevation, location and time of year. But it would be interesting to see which of theses ends up being the most important and which ends up being less important. This is kind of a barebones idea but I feel like some form of comparison with statistics with our data against an existing model could also be interesting in deciphering how strong a CSO model could be.\n",
    "\n",
    "* In terms of the original tests we have been running I still need to do maximum depth cutoff tests with the max snowprobe length to rule out observations that are too large. It's worth noting that this may only be smart to do on MountainHub data as it appears as though SnowPilot has it's own form of depth collection with their own procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ed4e3-8657-4740-94d6-04cd10e1e6d3",
   "metadata": {},
   "source": [
    "## Some things that I need to do very very soon...\n",
    "\n",
    "* I need to create some repositories for everything I've done so far. I should have all the tools and experience to do this but I've been somewhat procrasinating this. I will have this done by Saturday.\n",
    "\n",
    "* Go back and take a look at some hackweeks, especially in regards to projections. It will be very helpful if I could start laying down some vectors of models if we end up attaining them.\n",
    "\n",
    "* Familiarize myself with some python statistics packages. I have my eyes on some that would serve a similar purpose to R and wouldn't have much of a problem sorting through data of our magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150a76a-5558-41b4-9d72-ecfac88c85ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cso_obsdata]",
   "language": "python",
   "name": "conda-env-cso_obsdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
